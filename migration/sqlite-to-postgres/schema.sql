-- ============================================================================
-- BGPKIT Broker PostgreSQL Schema
-- ============================================================================
-- This schema is designed for the BGPKIT Broker service, migrated from SQLite.
-- 
-- Key Design Decisions:
-- 1. Native TIMESTAMPTZ for all timestamps (no BIGINT compatibility layer)
-- 2. Normalized design with collectors table to reduce storage
-- 3. Optimized composite indexes aligned with actual query patterns
-- 4. Dedicated latest table with UPSERT for real-time updates
-- 5. NULL for unknown sizes (0 is not a valid size)
-- 6. Partitioning-ready design (can add later with minimal changes)
--
-- Tables:
-- - collectors: BGP collector metadata (~80 rows)
-- - files: Main table storing BGP archive file records (~55M rows)
-- - latest: Tracks latest file per collector/type (~160 rows, real-time updates)
-- - meta: Stores metadata about database updates
--
-- Query Patterns Optimized:
-- 1. Time-range queries on files (ts_start to ts_end)
-- 2. Filtering by collector, project, and data type
-- 3. Pagination with ORDER BY ts ASC
-- 4. COUNT queries for pagination totals
-- 5. Latest file lookups per collector/type (O(1) via latest table)
--
-- Usage:
-- For managed PostgreSQL (Neon, Supabase, etc.), you may need to:
-- 1. Create the schema first: CREATE SCHEMA IF NOT EXISTS bgpkit;
-- 2. Set search_path: SET search_path TO bgpkit;
-- 3. Then run this script
-- ============================================================================

-- Drop existing objects if they exist (for clean re-initialization)
DROP VIEW IF EXISTS latest_view CASCADE;
DROP TABLE IF EXISTS latest CASCADE;
DROP VIEW IF EXISTS files_view CASCADE;
DROP TABLE IF EXISTS meta CASCADE;
DROP TABLE IF EXISTS files CASCADE;
DROP TABLE IF EXISTS collectors CASCADE;

-- ============================================================================
-- ENUM: data_type
-- ============================================================================
-- BGP data types. Using enum instead of lookup table for:
-- - Type safety at database level
-- - No joins needed for type filtering
-- - Compact storage (4 bytes vs 8 for bigint FK)
-- ============================================================================
DROP TYPE IF EXISTS data_type CASCADE;
CREATE TYPE data_type AS ENUM ('rib', 'updates');

-- ============================================================================
-- ENUM: project_type  
-- ============================================================================
-- BGP data sources. Using enum for same benefits as data_type.
-- ============================================================================
DROP TYPE IF EXISTS project_type CASCADE;
CREATE TYPE project_type AS ENUM ('ripe-ris', 'route-views');

-- ============================================================================
-- Table: collectors
-- ============================================================================
-- Stores BGP collector metadata from RouteViews and RIPE RIS projects.
-- ~80+ collectors, small table with infrequent updates.
--
-- Design rationale:
-- - Unique on (project, name) as collectors are unique within a project
-- - Uses enums for project type (type safety, no joins)
-- - ID is auto-generated but can be manually specified if needed
-- - Collectors should never be deleted (ON DELETE RESTRICT everywhere)
--
-- Columns:
-- - id: Internal unique identifier (auto-generated)
-- - project: Either 'ripe-ris' or 'route-views' (enum)
-- - name: Collector name (e.g., 'rrc00', 'route-views2')
-- - url: Base URL for data access
-- - updates_interval: Update frequency in seconds (300 for RIS, 900 for RV)
-- ============================================================================
CREATE TABLE collectors (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    project project_type NOT NULL,
    name TEXT NOT NULL,
    url TEXT,
    updates_interval INTEGER NOT NULL DEFAULT 900,
    UNIQUE (project, name)
);

-- ============================================================================
-- Table: files
-- ============================================================================
-- Main data table storing BGP archive file records.
-- This is the largest table (~55M rows) and requires careful index design.
--
-- Design rationale:
-- - Uses native TIMESTAMPTZ for all time handling (no epoch conversion)
-- - Uses enum for data_type (no joins, type safety)
-- - collector_id FK to reduce row size vs storing collector info inline
-- - NULL for unknown sizes (0 would be ambiguous)
-- - Partitioning-ready: PK includes ts for range partitioning compatibility
--
-- Columns:
-- - ts: Timestamp when BGP data was collected
-- - collector_id: Foreign key to collectors table
-- - data_type: Either 'rib' or 'updates' (enum)
-- - rough_size: Approximate file size in bytes (NULL if unknown)
-- - exact_size: Exact file size in bytes (NULL if unknown)
--
-- Storage estimate per row:
-- - ts: 8 bytes
-- - collector_id: 4 bytes
-- - data_type: 4 bytes (enum)
-- - rough_size: 8 bytes (nullable)
-- - exact_size: 8 bytes (nullable)
-- - Total: ~32 bytes + tuple header (~24 bytes) ≈ 56 bytes/row
-- - 55M rows ≈ 3GB base table size
-- ============================================================================
CREATE TABLE files (
    ts TIMESTAMPTZ NOT NULL,
    collector_id INTEGER NOT NULL REFERENCES collectors(id) ON DELETE RESTRICT,
    data_type data_type NOT NULL,
    rough_size BIGINT,  -- NULL means unknown
    exact_size BIGINT,  -- NULL means unknown
    PRIMARY KEY (ts, collector_id, data_type)
);

-- ============================================================================
-- Index Strategy for files table
-- ============================================================================
-- 
-- Primary Index: (ts, collector_id, data_type) - created by PK
-- This btree index supports:
-- - WHERE ts BETWEEN ... (leftmost column)
-- - ORDER BY ts ASC/DESC
-- - Uniqueness enforcement
--
-- Additional indexes for filtered queries:
-- ============================================================================

-- Index for: WHERE collector_id = ? AND data_type = ? AND ts BETWEEN ...
--            ORDER BY ts
-- This covers the common pattern of filtering by collector/type then time range
CREATE INDEX idx_files_collector_type_ts 
    ON files (collector_id, data_type, ts);

-- Index for: WHERE data_type = ? AND ts BETWEEN ... ORDER BY ts
-- Supports global type filtering (e.g., "all RIBs in time range")
CREATE INDEX idx_files_type_ts 
    ON files (data_type, ts);

-- ============================================================================
-- View: files_view
-- ============================================================================
-- Denormalized view joining files with collectors.
-- This is the primary interface for search queries.
-- 
-- Provides collector name, url, project without storing them in files table.
-- Enum types are cast to text for API compatibility.
-- ============================================================================
CREATE VIEW files_view AS
SELECT
    f.ts,
    f.rough_size,
    f.exact_size,
    f.data_type::text AS type,
    c.name AS collector_name,
    c.url AS collector_url,
    c.project::text AS project_name,
    c.updates_interval
FROM files f
JOIN collectors c ON c.id = f.collector_id;

-- ============================================================================
-- Table: latest
-- ============================================================================
-- Tracks the most recent file for each collector/data_type combination.
-- Used for the /v3/latest API endpoint.
--
-- Design rationale:
-- - Dedicated table instead of materialized view for real-time updates
-- - Only ~160 rows (82 collectors × 2 types) - very small
-- - UPSERT after each batch insert - instant updates, no scanning 55M rows
-- - Same approach as SQLite implementation
--
-- Update strategy:
-- - Call upsert_latest() after each batch of inserts
-- - Update only when new ts > existing ts (keeps latest)
-- - O(1) update time regardless of files table size
-- ============================================================================
CREATE TABLE latest (
    collector_name TEXT NOT NULL,
    type TEXT NOT NULL,  -- 'rib' or 'updates'
    ts TIMESTAMPTZ NOT NULL,
    rough_size BIGINT,
    exact_size BIGINT,
    PRIMARY KEY (collector_name, type)
);

-- Index for fast lookups by collector
CREATE INDEX idx_latest_collector ON latest (collector_name);

-- ============================================================================
-- View: latest_view
-- ============================================================================
-- Denormalized view joining latest with collectors for API responses.
-- Provides collector url, project, interval without storing in latest table.
-- ============================================================================
CREATE VIEW latest_view AS
SELECT
    l.ts,
    l.rough_size,
    l.exact_size,
    l.type,
    l.collector_name,
    c.url AS collector_url,
    c.project::text AS project_name,
    c.updates_interval
FROM latest l
JOIN collectors c ON c.name = l.collector_name;

-- ============================================================================
-- Table: meta
-- ============================================================================
-- Stores metadata about database update operations.
-- Used for monitoring and debugging the crawl process.
-- Append-only table, grows with each update cycle.
--
-- Columns:
-- - id: Auto-generated primary key
-- - update_ts: Timestamp of when the update occurred
-- - update_duration: How long the update took in seconds
-- - insert_count: Number of new records inserted
--
-- Retention: Keep only last 30 days of entries (cleaned by cleanup_meta())
-- At 5-min intervals: ~8,640 rows/month, ~100KB storage
-- ============================================================================
CREATE TABLE meta (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    update_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    update_duration INTEGER NOT NULL DEFAULT 0,
    insert_count INTEGER NOT NULL DEFAULT 0
);

-- Index for efficient "latest update" queries
CREATE INDEX idx_meta_update_ts ON meta(update_ts DESC);

-- ============================================================================
-- Helper Functions
-- ============================================================================

-- Function to upsert latest entry for a collector/type
-- Updates only if the new timestamp is greater than existing
CREATE OR REPLACE FUNCTION upsert_latest(
    p_collector_name TEXT,
    p_type TEXT,
    p_ts TIMESTAMPTZ,
    p_rough_size BIGINT,
    p_exact_size BIGINT
) RETURNS void AS $$
BEGIN
    INSERT INTO latest (collector_name, type, ts, rough_size, exact_size)
    VALUES (p_collector_name, p_type, p_ts, p_rough_size, p_exact_size)
    ON CONFLICT (collector_name, type)
    DO UPDATE SET
        ts = CASE 
            WHEN EXCLUDED.ts > latest.ts THEN EXCLUDED.ts 
            ELSE latest.ts 
        END,
        rough_size = CASE 
            WHEN EXCLUDED.ts > latest.ts THEN EXCLUDED.rough_size 
            ELSE latest.rough_size 
        END,
        exact_size = CASE 
            WHEN EXCLUDED.ts > latest.ts THEN EXCLUDED.exact_size 
            ELSE latest.exact_size 
        END;
END;
$$ LANGUAGE plpgsql;

-- Function to bootstrap latest table from files (one-time operation)
-- Use after initial data migration
CREATE OR REPLACE FUNCTION bootstrap_latest()
RETURNS INTEGER AS $$
DECLARE
    row_count INTEGER;
BEGIN
    -- Clear existing data
    TRUNCATE latest;
    
    -- Insert latest file for each collector/type combination
    INSERT INTO latest (collector_name, type, ts, rough_size, exact_size)
    SELECT DISTINCT ON (c.name, f.data_type)
        c.name AS collector_name,
        f.data_type::text AS type,
        f.ts,
        f.rough_size,
        f.exact_size
    FROM files f
    JOIN collectors c ON c.id = f.collector_id
    ORDER BY c.name, f.data_type, f.ts DESC;
    
    GET DIAGNOSTICS row_count = ROW_COUNT;
    RETURN row_count;
END;
$$ LANGUAGE plpgsql;

-- Function to clean up old meta entries (keep last 30 days)
-- Call periodically (e.g., daily via cron or after each update)
CREATE OR REPLACE FUNCTION cleanup_meta(retention_days INTEGER DEFAULT 30)
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    DELETE FROM meta 
    WHERE update_ts < NOW() - (retention_days || ' days')::INTERVAL;
    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Performance Considerations
-- ============================================================================
--
-- 1. VACUUM ANALYZE: Run after initial data load to update statistics
--    ANALYZE files;
--    ANALYZE collectors;
--
-- 2. Connection pooling: Use PgBouncer or similar for production
--
-- 3. Memory settings: Tune shared_buffers, work_mem for large result sets
--    Recommended: shared_buffers = 25% of RAM, work_mem = 256MB
--
-- 4. Table partitioning (future):
--    If needed, convert to partitioned table:
--
--    -- Step 1: Create partitioned table
--    CREATE TABLE files_partitioned (...) PARTITION BY RANGE (ts);
--    
--    -- Step 2: Create partitions (e.g., by year)
--    CREATE TABLE files_2020 PARTITION OF files_partitioned
--        FOR VALUES FROM ('2020-01-01') TO ('2021-01-01');
--    
--    -- Step 3: Migrate data and swap tables
--
-- 5. CLUSTER: Periodically run to physically reorder rows by PK index:
--    CLUSTER files USING files_pkey;
--    (Requires exclusive lock, schedule during maintenance windows)
--
-- 6. Latest table updates:
--    The latest table is updated via UPSERT after each batch insert.
--    This is O(1) per collector/type - no scanning of files table needed.
--    For initial setup, call: SELECT bootstrap_latest();
--
-- 7. Meta table cleanup:
--    Call periodically to prevent unbounded growth:
--    SELECT cleanup_meta(30);  -- Keep last 30 days
--    At 5-min intervals, 30 days = ~8,640 rows (~100KB)
--
-- ============================================================================
