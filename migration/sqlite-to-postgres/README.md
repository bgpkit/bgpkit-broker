# SQLite to PostgreSQL Migration for BGPKIT Broker

This directory contains tools and documentation for migrating the BGPKIT Broker database from SQLite to PostgreSQL.

## Overview

The BGPKIT Broker uses a database to store BGP archive file metadata. This migration enables:
- Better scalability for large datasets (55M+ rows)
- Concurrent access and connection pooling
- Native `TIMESTAMPTZ` for all time handling
- PostgreSQL enums for type safety (`data_type`, `project_type`)
- Dedicated `latest` table with real-time UPSERT updates
- Stronger data integrity with CHECK constraints and proper FKs

## Database Schema

### Tables

| Table | Rows (approx) | Description |
|-------|---------------|-------------|
| `collectors` | 80+ | BGP collector metadata (RouteViews, RIPE RIS) |
| `files` | 55M+ | Main data table - BGP archive file records |
| `latest` | ~160 | Latest file per collector/type (real-time updates via UPSERT) |
| `meta` | Growing | Update operation metadata |

### Enums

```sql
CREATE TYPE data_type AS ENUM ('rib', 'updates');
CREATE TYPE project_type AS ENUM ('ripe-ris', 'route-views');
```

Using enums instead of lookup tables provides:
- Type safety at database level
- No joins needed for type/project filtering
- Compact storage (4 bytes vs 8 for bigint FK)

### Schema Design

#### `collectors` Table
```sql
CREATE TABLE collectors (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    project project_type NOT NULL,
    name TEXT NOT NULL,
    url TEXT,
    updates_interval INTEGER NOT NULL DEFAULT 900,
    UNIQUE (project, name)
);
```
- Unique on `(project, name)` - collectors are unique within a project
- Auto-generated IDs with manual override capability
- Collectors are never deleted (RESTRICT everywhere)

#### `files` Table (Main Data Table)
```sql
CREATE TABLE files (
    ts TIMESTAMPTZ NOT NULL,
    collector_id INTEGER NOT NULL REFERENCES collectors(id) ON DELETE RESTRICT,
    data_type data_type NOT NULL,
    rough_size BIGINT,  -- NULL means unknown
    exact_size BIGINT,  -- NULL means unknown
    PRIMARY KEY (ts, collector_id, data_type)
);
```
- Native `TIMESTAMPTZ` for all time handling
- Enum for `data_type` (no joins, type safety)
- NULL for unknown sizes (0 is not a valid size)
- Partitioning-ready: PK includes ts for range partitioning

**Storage estimate:**
- ~56 bytes per row (32 bytes data + 24 bytes tuple header)
- 55M rows ≈ 3GB base table size

#### `latest` Table

```sql
CREATE TABLE latest (
    collector_name TEXT NOT NULL,
    type TEXT NOT NULL,  -- 'rib' or 'updates'
    ts TIMESTAMPTZ NOT NULL,
    rough_size BIGINT,
    exact_size BIGINT,
    PRIMARY KEY (collector_name, type)
);
```

**Design rationale:**
- Dedicated table instead of materialized view for **real-time updates**
- Only ~160 rows (82 collectors × 2 types) - very small
- UPSERT after each batch insert - instant updates, no scanning 55M rows
- Same approach as SQLite implementation
- O(1) update time regardless of files table size

**Update mechanism:**
```sql
-- Automatic UPSERT (only updates if new ts > existing ts)
INSERT INTO latest (collector_name, type, ts, rough_size, exact_size)
VALUES (...)
ON CONFLICT (collector_name, type)
DO UPDATE SET
    ts = CASE WHEN EXCLUDED.ts > latest.ts THEN EXCLUDED.ts ELSE latest.ts END,
    rough_size = CASE WHEN EXCLUDED.ts > latest.ts THEN EXCLUDED.rough_size ELSE latest.rough_size END,
    exact_size = CASE WHEN EXCLUDED.ts > latest.ts THEN EXCLUDED.exact_size ELSE latest.exact_size END;
```

#### `meta` Table
```sql
CREATE TABLE meta (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    update_ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    update_duration INTEGER NOT NULL DEFAULT 0,
    insert_count INTEGER NOT NULL DEFAULT 0
);
```

#### `files_view` View
```sql
CREATE VIEW files_view AS
SELECT
    f.ts, f.rough_size, f.exact_size,
    f.data_type::text AS type,
    c.name AS collector_name,
    c.url AS collector_url,
    c.project::text AS project_name,
    c.updates_interval
FROM files f
JOIN collectors c ON c.id = f.collector_id;
```
Primary search interface. Joins files with collector metadata.

#### `latest_view` View
```sql
CREATE VIEW latest_view AS
SELECT
    l.ts, l.rough_size, l.exact_size, l.type,
    l.collector_name,
    c.url AS collector_url,
    c.project::text AS project_name,
    c.updates_interval
FROM latest l
JOIN collectors c ON c.name = l.collector_name;
```
Denormalized view for API responses with collector details.

### Index Strategy

| Index | Columns | Purpose |
|-------|---------|---------|
| PK | `(ts, collector_id, data_type)` | Primary key, time-range queries |
| `idx_files_collector_type_ts` | `(collector_id, data_type, ts)` | Filtered queries by collector/type |
| `idx_files_type_ts` | `(data_type, ts)` | Global type filtering |
| `idx_meta_update_ts` | `(update_ts DESC)` | Recent update lookups |
| `idx_latest_collector` | `(collector_name)` | Fast collector lookups in latest |

**Design rationale:**
- PK index supports `WHERE ts BETWEEN ...` and `ORDER BY ts`
- Composite indexes aligned with actual query patterns
- Small tables don't need additional indexes

## Migration Process

### Prerequisites

1. Python 3.8+ with pip
2. PostgreSQL 12+ database
3. Source SQLite database file

### Installation

```bash
cd migration/sqlite-to-postgres
pip install -r requirements.txt
```

### Environment Variables

Set the following environment variables for PostgreSQL connection:

```bash
export BROKER_DATABASE_HOST=localhost      # PostgreSQL host (default: localhost)
export BROKER_DATABASE_PORT=5432           # PostgreSQL port (default: 5432)
export BROKER_DATABASE_USERNAME=postgres   # PostgreSQL username (required)
export BROKER_DATABASE_PASSWORD=secret     # PostgreSQL password (required)
export BROKER_DATABASE=bgpkit_broker       # PostgreSQL database name (required)
export BROKER_DATABASE_SCHEMA=bgpkit       # Optional schema name (for managed PG)
export BROKER_DATABASE_SSLMODE=require     # SSL mode: disable, prefer, require (default: require)
```

Both the migration script and Rust application use the same `BROKER_DATABASE_*` prefix for consistency.

### Running the Migration

```bash
# Migrate last 30 days (default - fast for testing)
python migrate_to_postgres.py ./bgpkit_broker.sqlite3

# Migrate all data (full migration)
python migrate_to_postgres.py --all ./bgpkit_broker.sqlite3

# Migrate last 7 days
python migrate_to_postgres.py --days 7 ./bgpkit_broker.sqlite3

# Use a custom schema (for managed PostgreSQL)
python migrate_to_postgres.py --schema bgpkit ./bgpkit_broker.sqlite3

# Skip schema creation (if already exists)
python migrate_to_postgres.py --skip-schema --all ./db.sqlite3

# Adjust batch size for memory-constrained environments
python migrate_to_postgres.py --batch-size 5000 --all ./db.sqlite3
```

### Command Line Options

| Option | Description |
|--------|-------------|
| `--all` | Migrate all data (default: last 30 days only) |
| `--days N` | Migrate last N days of data |
| `--schema NAME` | PostgreSQL schema name (e.g., 'bgpkit'). Creates if needed. |
| `--skip-schema` | Skip schema creation (use if schema already exists) |
| `--skip-files` | Skip files table migration (for testing) |
| `--skip-refresh` | Skip latest table bootstrap (populate manually later) |
| `--batch-size N` | Rows per batch (default: 10000) |

### Bootstrapping the Latest Table

After initial migration, the `latest` table needs to be populated from the `files` table.
This is a one-time operation:

```bash
# Check latest table status only
python refresh_views.py --status-only

# Bootstrap the latest table (scans files to find latest per collector/type)
python refresh_views.py
```

For a 55M row files table, the bootstrap operation typically takes 1-5 minutes.
After initial setup, updates are instant via UPSERT (no scanning required).

### Managed PostgreSQL (Neon, Supabase, etc.)

If you get a "permission denied for schema public" error, use the `--schema` option
to create objects in a custom schema:

```bash
# Use a custom schema (recommended for managed PostgreSQL)
python migrate_to_postgres.py --schema bgpkit ./db.sqlite3 "postgresql://user:pass@host/db"
```

This will:
1. Create the schema if it doesn't exist (`CREATE SCHEMA IF NOT EXISTS bgpkit`)
2. Set the search_path to use that schema
3. Create all tables and views within that schema

### Progress Tracking

The migration script provides detailed progress information:
- Current row count and percentage
- Elapsed time and estimated remaining time
- Processing rate (rows/second)
- Time range of data being migrated

Example output:
```
Migrating files table...
  Filter: last 30 days (since 2024-10-30)
  Files to migrate: 2,345,678
  Time range: 2024-10-30 00:00 to 2024-11-29 04:00
Migrating files: 100%|████████████| 2.3M/2.3M [05:23<00:00, 7.2K/s]
  Completed: 2,345,678 rows in 5.4m (7.2K/s)
  Migrated: 2,345,678 files
```

### Migration Steps

1. **Schema Creation**: Creates enums, tables, indexes, views
2. **Collectors Migration**: Migrates ~80 collector records (with new ID mapping)
3. **Files Migration**: Batched migration with progress tracking:
   - Timestamp conversion (epoch → TIMESTAMPTZ)
   - Collector ID mapping (old → new)
   - Type conversion (type_id → enum)
   - Size conversion (0 → NULL)
4. **Latest Bootstrap**: Populates latest table from files data
5. **Meta Migration**: Migrates update history with timestamp conversion
6. **ANALYZE**: Updates PostgreSQL statistics
7. **Verification**: Compares row counts and time ranges

### Expected Duration

For full migration (55M files):
- Local SSD: 30-60 minutes
- Local HDD: 2-4 hours
- Cloud database: 2-6 hours (network dependent)

For 30-day migration (~2-3M files):
- Local: 3-5 minutes
- Cloud: 10-20 minutes

## Post-Migration

### Verify Data Integrity

```sql
-- Check row counts
SELECT 'files' as table_name, COUNT(*) as count FROM files
UNION ALL SELECT 'collectors', COUNT(*) FROM collectors
UNION ALL SELECT 'latest', COUNT(*) FROM latest
UNION ALL SELECT 'meta', COUNT(*) FROM meta;

-- Verify time range
SELECT MIN(ts), MAX(ts) FROM files;

-- Check data distribution
SELECT c.project, f.data_type, COUNT(*) 
FROM files f 
JOIN collectors c ON f.collector_id = c.id 
GROUP BY c.project, f.data_type;

-- Test files_view
SELECT * FROM files_view 
WHERE ts >= '2024-01-01' AND ts < '2024-01-02'
LIMIT 10;
```

### Updating Latest Table

The `latest` table is updated automatically via UPSERT after each batch of file inserts.
No manual refresh is needed during normal operation.

For recovery or re-initialization:
```sql
-- Bootstrap latest table from files (one-time or recovery)
SELECT bootstrap_latest();
```

### Performance Tuning

```sql
-- postgresql.conf recommendations for dedicated PostgreSQL
shared_buffers = '4GB'           -- 25% of RAM
effective_cache_size = '12GB'    -- 75% of RAM
work_mem = '256MB'               -- For sorting/hashing
maintenance_work_mem = '1GB'     -- For VACUUM/ANALYZE
random_page_cost = 1.1           -- For SSD storage
```

### Application Tuning Environment Variables

For serverless databases (PlanetScale, Neon, Supabase), use these environment variables
to tune connection and write behavior:

| Variable | Default | Description |
|----------|---------|-------------|
| `BROKER_DATABASE_POOL_SIZE` | 3 | Max database connections in the pool. Keep low for serverless. |
| `BROKER_INSERT_BATCH_SIZE` | 500 | Rows per INSERT batch. Smaller batches reduce transaction time. |
| `BROKER_CRAWL_PARALLELISM` | 2 | Number of collectors to crawl in parallel. Lower = fewer concurrent DB writes. |

**Recommended settings for serverless databases:**
```bash
export BROKER_DATABASE_POOL_SIZE=3
export BROKER_INSERT_BATCH_SIZE=500
export BROKER_CRAWL_PARALLELISM=2
```

**Recommended settings for dedicated PostgreSQL:**
```bash
export BROKER_DATABASE_POOL_SIZE=10
export BROKER_INSERT_BATCH_SIZE=1000
export BROKER_CRAWL_PARALLELISM=5
```

### Partitioning (Future)

When table grows beyond 100M rows:
```sql
-- Convert to partitioned table (requires migration)
CREATE TABLE files_partitioned (...) PARTITION BY RANGE (ts);

CREATE TABLE files_2024 PARTITION OF files_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

## Files in This Directory

| File | Description |
|------|-------------|
| `README.md` | This documentation |
| `schema.sql` | PostgreSQL schema definition |
| `migrate_to_postgres.py` | Python migration script (data migration) |
| `migrate_matview_to_table.py` | Convert materialized view to table-based `latest` |
| `refresh_views.py` | Bootstrap/refresh the `latest` table |
| `requirements.txt` | Python dependencies |
| `QUESTIONS.md` | Design decisions and rationale |

### Fixing Materialized View to Table Migration

If you migrated data using an older version of the schema that used a materialized view 
for `latest`, run this script to convert to the table-based approach:

```bash
# Set environment variables
export BROKER_DATABASE_HOST=your-host.example.com
export BROKER_DATABASE_PORT=5432
export BROKER_DATABASE_USERNAME=your_user
export BROKER_DATABASE_PASSWORD=your_password
export BROKER_DATABASE=your_database

# Check current schema (dry run)
python migrate_matview_to_table.py --dry-run

# Run the migration
python migrate_matview_to_table.py

# Or skip bootstrap if you want to run it manually
python migrate_matview_to_table.py --skip-bootstrap
```

This script will:
1. Drop the materialized view if it exists
2. Create the `latest` table with proper primary key
3. Create `upsert_latest()` function for real-time updates
4. Create `bootstrap_latest()` function for recovery/initialization
5. Bootstrap the table from existing `files` data

## Troubleshooting

### Out of Memory
Reduce batch size: `--batch-size 1000`

### Connection Timeout
Increase PostgreSQL timeout settings.

### Duplicate Key Errors
Safe to re-run - uses `ON CONFLICT DO NOTHING`.

### Slow Queries After Migration
Run `ANALYZE files;` to update statistics.

### Serverless Database Connection Issues

**"expected to read 5 bytes, got 0 bytes at EOF"**
- The connection was closed by the serverless database (idle timeout or restart)
- The application has built-in retry logic for transient errors
- Reduce `BROKER_DATABASE_POOL_SIZE` and `BROKER_CRAWL_PARALLELISM`

**"server login has been failing"**
- Database instance is restarting (common with serverless under memory pressure)
- Reduce concurrent writes: `BROKER_CRAWL_PARALLELISM=1`
- Use smaller batches: `BROKER_INSERT_BATCH_SIZE=200`

**"branch does not exist"** (PlanetScale specific)
- Database branch was deleted or renamed while connection was active
- Check PlanetScale dashboard for branch status
- Reconnect with updated credentials if branch changed

**Memory exhaustion causing restarts**
- Reduce `BROKER_DATABASE_POOL_SIZE` to 2 or 3
- Reduce `BROKER_CRAWL_PARALLELISM` to 1
- Use smaller `BROKER_INSERT_BATCH_SIZE` (200-500)
- Consider upgrading to a larger serverless tier
